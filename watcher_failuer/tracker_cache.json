{": 'sudo mkdir -p /sys/kernel/config/nvmet/subsystems/lv_1 && echo 1 | sudo tee /sys/kernel/config/nvmet/subsystems/lv_1/attr_allo/w_any_host && sudo mkdir -p /sys/kernel/config/nvmet/subsystems/lv_1/namespaces/1 && echo /dev/vg_nvme/lv_1 | sudo tee /sys/kernel/config/nvmet/subsystems/lv_1/namespaces/1/device_path && echo 1 | sudo tee /sys/kernel/config/nvmet/subsystems/lv_1/namespaces/1/enable && sudo ln -s /sys/kernel/config/nvmet/subsystems/lv_1 /sys/kernel/config/nvmet/ports/1/subsystems/lv_1 && sudo nvme connect -t loop -n lv_1 -q hostnqn'": {"link": "https://tracker.ceph.com/issues/67553", "issue_id": 67553}, "Failed to fetch package version from https://shaman.ceph.com/api/search/?status=ready&project=ceph&flavor=default&distros=centos%2F9%2Fx86_64&ref=pacific": {"link": "https://tracker.ceph.com/issues/59193", "issue_id": 59193}, "Command failed (workunit test cephadm/test_repos.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/workunits/cephadm/test_repos.sh'": {"link": "https://tracker.ceph.com/issues/67555", "issue_id": 67555}, "Command failed (workunit test cephadm/test_iscsi_pids_limit.sh) on smithi000 with status 2: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/workunits/cephadm/test_iscsi_pids_limit.sh'": {"link": "https://tracker.ceph.com/issues/66998", "issue_id": 66998}, "Command failed (workunit test cephadm/test_cephadm.sh) on smithi000 with status 125: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/workunits/cephadm/test_cephadm.sh'": {"link": "https://tracker.ceph.com/issues/56419", "issue_id": 56419}, "Command failed (workunit test cephadm/test_dashboard_e2e.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/workunits/cephadm/test_dashboard_e2e.sh'": {"link": "https://tracker.ceph.com/issues/67811", "issue_id": 67811}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay.ceph.io/ceph-ci/ceph:pacific pull'": {"link": "https://tracker.ceph.com/issues/67561", "issue_id": 67561}, ": \"sudo TESTDIR=/home/ubuntu/cephtest bash -c 'ceph_test_cls_rbd --gtest_filter=-TestClsRbd.get_features:TestClsRbd.parents:TestClsRbd.mirror'\"": {}, "Command crashed: 'sudo TESTDIR=/home/ubuntu/cephtest bash -c ceph_test_lazy_omap_stats'": {"link": "https://tracker.ceph.com/issues/59196", "issue_id": 59196}, ": 'sudo TESTDIR=/home/ubuntu/cephtest bash -c \\'mkdir $TESTDIR/archive/ostest && cd $TESTDIR/archive/ostest && ulimit -Sn 16384 && CEPH_ARGS=\"--no-log-to-stderr --log-file $TESTDIR/archive/ceph_test_objectstore.log --debug-bluestore 20\" ceph_test_objectstore --gtest_filter=*/2:-*SyntheticMatrixC* --gtest_catch_exceptions=0\\''": {"link": "https://tracker.ceph.com/issues/54438", "issue_id": 54438}, "Command failed (workunit test cephtool/test.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/workunits/cephtool/test.sh'": {"link": "https://tracker.ceph.com/issues/38388", "issue_id": 38388}, "Command failed (workunit test cephadm/test_cephadm.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/workunits/cephadm/test_cephadm.sh'": {"link": "https://tracker.ceph.com/issues/56419", "issue_id": 56419}, "Command failed (workunit test mon/mon-cluster-log.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/standalone/mon/mon-cluster-log.sh'": {"link": "https://tracker.ceph.com/issues/67282", "issue_id": 67282}, "None": {"link": "https://tracker.ceph.com/issues/67524", "issue_id": 67524}, "\"2024-10-03T21:54:15.792882+0000 mon.a (mon.0) 592 : cluster [WRN] Health check failed: 2 osds down (OSD_DOWN)\" in cluster log": {}, "['E: Could not get lock /var/lib/dpkg/lock-frontend. It is held by process 7609 (apt-get)']": {}, "['E: Could not get lock /var/lib/dpkg/lock-frontend. It is held by process 7086 (apt-get)']": {}, "Command failed (workunit test scrub/osd-scrub-dump.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/standalone/scrub/osd-scrub-dump.sh'": {}, ": '/home/ubuntu/cephtest/cbt/cbt.py -a /home/ubuntu/cephtest/archive/cbt /home/ubuntu/cephtest/archive/cbt/cbt_config.yaml'": {}, "b'2024-10-07T03:31:56.097 DEBUG:teuthology.exit:Finished running handlers'": {}, "Command failed (workunit test rbd/crimson/test_crimson_librbd.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 CRIMSON_COMPAT=1 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/workunits/rbd/crimson/test_crimson_librbd.sh'": {}, "Command failed (workunit test cls/test_cls_rbd.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 CRIMSON_COMPAT=1 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/workunits/cls/test_cls_rbd.sh'": {}, ": 'sudo mkdir -p /sys/kernel/config/nvmet/subsystems/lv_1 && echo 1 | sudo tee /sys/kernel/config/nvmet/subsystems/lv_1/attr_allow_any_host && sudo mkdir -p /sys/kernel/config/nvmet/subsystems/lv_1/namespaces/1 && echo /dev/vg_nvme/lv_1 | sudo tee /sys/kernel/config/nvmet/subsystems/lv_1/namespaces/1/device_path && echo 1 | sudo tee /sys/kernel/config/nvmet/subsystems/lv_1/namespaces/1/enable && sudo ln -s /sys/kernel/config/nvmet/subsystems/lv_1 /sys/kernel/config/nvmet/ports/1/subsystems/lv_1 && sudo nvme connect -t loop -n lv_1 -q hostnqn'": {"link": "https://tracker.ceph.com/issues/67684", "issue_id": 67684}, "Command failed (workunit test osd-backfill/osd-backfill-space.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/standalone/osd-backfill/osd-backfill-space.sh'": {}, "Test failure: test_list_enabled_module (tasks.mgr.dashboard.test_mgr_module.MgrModuleTest)": {}, "Command failed (workunit test rados/test.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 ALLOW_TIMEOUTS=1 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 6h /home/ubuntu/cephtest/clone.client.0/qa/workunits/rados/test.sh'": {"link": "https://tracker.ceph.com/issues/40119", "issue_id": 40119}, "Command failed (workunit test rados/test.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/workunits/rados/test.sh'": {"link": "https://tracker.ceph.com/issues/40119", "issue_id": 40119}, "\"2024-10-13T17:35:13.384774+0000 mon.a (mon.0) 692 : cluster [WRN] Health check failed: 1 osds down (OSD_DOWN)\" in cluster log": {}, "\"2024-10-17T21:10:00.000162+0000 mon.a (mon.0) 1289 : cluster [WRN]     pg 2.0 is active+recovering+undersized+degraded+remapped, acting [7,9]\" in cluster log": {}, "b'2024-10-14T00:39:46.907 DEBUG:teuthology.exit:Finished running handlers'": {}, "Command failed (workunit test rados/test_rados_tool.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/workunits/rados/test_rados_tool.sh'": {}, "reached maximum tries (800) after waiting for 4800 seconds": {}, "b'2024-10-18T04:43:38.141 DEBUG:teuthology.exit:Finished running handlers'": {}, "Command failed (workunit test scrub/osd-scrub-test.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/standalone/scrub/osd-scrub-test.sh'": {"link": "https://tracker.ceph.com/issues/68579", "issue_id": 68579}, "Command failed (workunit test osd/osd-bluefs-volume-ops.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/standalone/osd/osd-bluefs-volume-ops.sh'": {}, "\"2024-10-14T11:37:17.425038+0000 mon.a (mon.0) 659 : cluster [WRN] Health check failed: 1 pool(s) do not have an application enabled (POOL_APP_NOT_ENABLED)\" in cluster log": {}, "reached maximum tries (50) after waiting for 300 seconds": {"link": "https://tracker.ceph.com/issues/16803", "issue_id": 16803}, "b'2024-10-14T18:19:02.664 DEBUG:teuthology.exit:Finished running handlers'": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay-quay-quay.apps.os.sepia.ceph.com/ceph-ci/ceph:057fc3e0953d1d792acd015e8d39e000d1301b13 shell -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring --fsid b0b4ef0a-8a20-11ef-bb99-d5e06f7e0c9a -- bash -c \\'set -e\\nset -x\\nwhile true; do TOKEN=$(ceph rgw realm tokens | jq -r \\'\"\\'\"\\'.[0].token\\'\"\\'\"\\'); echo $TOKEN; if [ \"$TOKEN\" != \"master zone has no endpoint\" ]; then break; fi; sleep 5; done\\nTOKENS=$(ceph rgw realm tokens)\\necho $TOKENS | jq --exit-status \\'\"\\'\"\\'.[0].realm == \"myrealm1\"\\'\"\\'\"\\'\\necho $TOKENS | jq --exit-status \\'\"\\'\"\\'.[0].token\\'\"\\'\"\\'\\nTOKEN_JSON=$(ceph rgw realm tokens | jq -r \\'\"\\'\"\\'.[0].token\\'\"\\'\"\\' | base64 --decode)\\necho $TOKEN_JSON | jq --exit-status \\'\"\\'\"\\'.realm_name == \"myrealm1\"\\'\"\\'\"\\'\\necho $TOKEN_JSON | jq --exit-status \\'\"\\'\"\\'.endpoint | test(\"http://.+:\\\\\\\\d+\")\\'\"\\'\"\\'\\necho $TOKEN_JSON | jq --exit-status \\'\"\\'\"\\'.realm_id | test(\"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$\")\\'\"\\'\"\\'\\necho $TOKEN_JSON | jq --exit-status \\'\"\\'\"\\'.access_key\\'\"\\'\"\\'\\necho $TOKEN_JSON | jq --exit-status \\'\"\\'\"\\'.secret\\'\"\\'\"\\'\\n\\''": {}, "\"2024-10-14T07:10:00.000167+0000 mon.a (mon.0) 1987 : cluster [WRN]     osd.9 (root=default,host=smithi000) is down\" in cluster log": {}, ": 'yes | sudo mkfs.xfs -f -i size=2048 /dev/vg_nvme/lv_2'": {"link": "https://tracker.ceph.com/issues/67860", "issue_id": 67860}, "Command failed (workunit test cls/test_cls_rbd.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=reef TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/workunits/cls/test_cls_rbd.sh'": {}, "\"2024-10-14T06:36:26.012187+0000 osd.8 (osd.8) 175 : cluster [ERR] 3.21 scrub : stat mismatch, got 4/3 objects, 3/2 clones, 4/3 dirty, 0/0 omap, 0/0 pinned, 0/0 hit_set_archive, 0/0 whiteouts, 9614594/9614594 bytes, 0/0 manifest objects, 0/0 hit_set_archive bytes.\" in cluster log": {}, "b'2024-10-14T15:38:40.928 DEBUG:teuthology.exit:Finished running handlers'": {}, "timeout expired in wait_until_healthy": {"link": "https://tracker.ceph.com/issues/64280", "issue_id": 64280}, ": 'sudo fuser -v /var/lib/dpkg/lock-frontend'": {}, "Command failed (workunit test rbd/test_librbd_python.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=quincy TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 RBD_FEATURES=61 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/workunits/rbd/test_librbd_python.sh'": {"link": "https://tracker.ceph.com/issues/63941", "issue_id": 63941}, "Command failed (workunit test dencoder/test-dencoder.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/workunits/dencoder/test-dencoder.sh'": {"link": "https://tracker.ceph.com/issues/66658", "issue_id": 66658}, "\"2024-10-15T21:06:13.816695+0000 osd.4 (osd.4) 3 : cluster [WRN] OSD bench result of 980.672224 IOPS is not within the threshold limit range of 3000.000000 IOPS and 80000.000000 IOPS for osd.4. IOPS capacity is unchanged at 21500.000000 IOPS. The recommendation is to establish the osd's IOPS capacity using other benchmark tools (e.g. Fio) and then override osd_mclock_max_capacity_iops_[hdd|ssd].\" in cluster log": {}, "HTTPConnectionPool(host='mira118.front.sepia.ceph.com', port=4000): Max retries exceeded with url: /cbt_performance (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe06a9582b0>: Failed to establish a new connection: [Errno 113] No route to host'))": {}, "HTTPConnectionPool(host='mira118.front.sepia.ceph.com', port=4000): Max retries exceeded with url: /cbt_performance (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f94a5afa200>: Failed to establish a new connection: [Errno 113] No route to host'))": {}, "Command failed (workunit test misc/test-ceph-helpers.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/standalone/misc/test-ceph-helpers.sh'": {}, "b'2024-10-24T20:34:44.856 DEBUG:teuthology.exit:Finished running handlers'": {}, "HTTPConnectionPool(host='mira118.front.sepia.ceph.com', port=4000): Max retries exceeded with url: /cbt_performance (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1ab45af520>: Failed to establish a new connection: [Errno 111] Connection refused'))": {}, "\"2024-10-28T06:56:28.614485+0000 mon.a (mon.0) 670 : cluster [WRN] Health check failed: 1 pool(s) do not have an application enabled (POOL_APP_NOT_ENABLED)\" in cluster log": {}, "\"2024-10-21T09:10:00.000143+0000 mon.a (mon.0) 1371 : cluster [WRN] Health detail: HEALTH_WARN 1 osds down; Degraded data redundancy: 135/2172 objects degraded (6.215%), 1 pg degraded, 1 pg undersized\" in cluster log": {}, "b'2024-10-28T14:39:40.973 DEBUG:teuthology.exit:Finished running handlers'": {}, "b'2024-10-21T14:46:19.448 DEBUG:teuthology.exit:Finished running handlers'": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay-quay-quay.apps.os.sepia.ceph.com/ceph-ci/ceph:ddc911a7bc2038b50046738508f2df9f2bdf27a6 shell -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring --fsid 1bd3bb64-9501-11ef-bb9c-d5e06f7e0c9a -- bash -c \\'set -e\\nset -x\\nwhile true; do TOKEN=$(ceph rgw realm tokens | jq -r \\'\"\\'\"\\'.[0].token\\'\"\\'\"\\'); echo $TOKEN; if [ \"$TOKEN\" != \"master zone has no endpoint\" ]; then break; fi; sleep 5; done\\nTOKENS=$(ceph rgw realm tokens)\\necho $TOKENS | jq --exit-status \\'\"\\'\"\\'.[0].realm == \"myrealm1\"\\'\"\\'\"\\'\\necho $TOKENS | jq --exit-status \\'\"\\'\"\\'.[0].token\\'\"\\'\"\\'\\nTOKEN_JSON=$(ceph rgw realm tokens | jq -r \\'\"\\'\"\\'.[0].token\\'\"\\'\"\\' | base64 --decode)\\necho $TOKEN_JSON | jq --exit-status \\'\"\\'\"\\'.realm_name == \"myrealm1\"\\'\"\\'\"\\'\\necho $TOKEN_JSON | jq --exit-status \\'\"\\'\"\\'.endpoint | test(\"http://.+:\\\\\\\\d+\")\\'\"\\'\"\\'\\necho $TOKEN_JSON | jq --exit-status \\'\"\\'\"\\'.realm_id | test(\"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$\")\\'\"\\'\"\\'\\necho $TOKEN_JSON | jq --exit-status \\'\"\\'\"\\'.access_key\\'\"\\'\"\\'\\necho $TOKEN_JSON | jq --exit-status \\'\"\\'\"\\'.secret\\'\"\\'\"\\'\\n\\''": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay-quay-quay.apps.os.sepia.ceph.com/ceph-ci/ceph:abf942b5e75f21ffdedcb6e210494d0df43aca3b shell -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring --fsid 0b10b6f4-8f8b-11ef-bb99-d5e06f7e0c9a -- bash -c \\'set -e\\nset -x\\nwhile true; do TOKEN=$(ceph rgw realm tokens | jq -r \\'\"\\'\"\\'.[0].token\\'\"\\'\"\\'); echo $TOKEN; if [ \"$TOKEN\" != \"master zone has no endpoint\" ]; then break; fi; sleep 5; done\\nTOKENS=$(ceph rgw realm tokens)\\necho $TOKENS | jq --exit-status \\'\"\\'\"\\'.[0].realm == \"myrealm1\"\\'\"\\'\"\\'\\necho $TOKENS | jq --exit-status \\'\"\\'\"\\'.[0].token\\'\"\\'\"\\'\\nTOKEN_JSON=$(ceph rgw realm tokens | jq -r \\'\"\\'\"\\'.[0].token\\'\"\\'\"\\' | base64 --decode)\\necho $TOKEN_JSON | jq --exit-status \\'\"\\'\"\\'.realm_name == \"myrealm1\"\\'\"\\'\"\\'\\necho $TOKEN_JSON | jq --exit-status \\'\"\\'\"\\'.endpoint | test(\"http://.+:\\\\\\\\d+\")\\'\"\\'\"\\'\\necho $TOKEN_JSON | jq --exit-status \\'\"\\'\"\\'.realm_id | test(\"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$\")\\'\"\\'\"\\'\\necho $TOKEN_JSON | jq --exit-status \\'\"\\'\"\\'.access_key\\'\"\\'\"\\'\\necho $TOKEN_JSON | jq --exit-status \\'\"\\'\"\\'.secret\\'\"\\'\"\\'\\n\\''": {}, "b'2024-10-27T10:33:04.526 DEBUG:teuthology.exit:Finished running handlers'": {}, "b'2024-10-20T11:16:10.542 DEBUG:teuthology.exit:Finished running handlers'": {}, "Command failed (workunit test rbd/crimson/test_crimson_librbd.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 RBD_FEATURES=61 CRIMSON_COMPAT=1 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/workunits/rbd/crimson/test_crimson_librbd.sh'": {}, "reached maximum tries (500) after waiting for 3000 seconds": {"link": "https://tracker.ceph.com/issues/39148", "issue_id": 39148}, "HTTPConnectionPool(host='mira118.front.sepia.ceph.com', port=4000): Max retries exceeded with url: /cbt_performance (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f7a43119f00>: Failed to establish a new connection: [Errno 111] Connection refused'))": {}, "\"2024-10-27T21:48:17.711365+0000 mon.a (mon.0) 252 : cluster [WRN] Health check failed: 2 OSD(s) experiencing slow operations in BlueStore (BLUESTORE_SLOW_OP_ALERT)\" in cluster log": {}, "b'2024-10-21T13:45:33.607 DEBUG:teuthology.exit:Finished running handlers'": {}, "HTTPConnectionPool(host='mira118.front.sepia.ceph.com', port=4000): Max retries exceeded with url: /cbt_performance (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd07d51e4d0>: Failed to establish a new connection: [Errno 111] Connection refused'))": {}, "b'2024-10-27T10:58:03.744 DEBUG:teuthology.exit:Finished running handlers'": {}, "b'2024-10-20T12:14:20.259 DEBUG:teuthology.exit:Finished running handlers'": {}, "Command failed (workunit test mon/mkfs.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/standalone/mon/mkfs.sh'": {}, "\"2024-10-21T13:49:50.124558+0000 mon.a (mon.0) 728 : cluster [WRN] Health check failed: 1 osds down (OSD_DOWN)\" in cluster log": {}, ": 'yes | sudo mkfs.xfs -f -i size=2048 -f /dev/vg_nvme/lv_2'": {"link": "https://tracker.ceph.com/issues/67860", "issue_id": 67860}, "['E: Could not get lock /var/lib/dpkg/lock-frontend. It is held by process 7836 (apt-get)']": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay-quay-quay.apps.os.sepia.ceph.com/ceph-ci/ceph:551832b7be80c004161610d3781b826779cce471 -v bootstrap --fsid d5023614-9d36-11ef-bb9c-d5e06f7e0c9a --config /home/ubuntu/cephtest/seed.ceph.conf --output-config /etc/ceph/ceph.conf --output-keyring /etc/ceph/ceph.client.admin.keyring --output-pub-ssh-key /home/ubuntu/cephtest/ceph.pub --mon-ip 172.21.15.71 --skip-admin-label && sudo chmod +r /etc/ceph/ceph.client.admin.keyring'": {}, "Test failure: test_standby (tasks.mgr.test_dashboard.TestDashboard)": {"link": "https://tracker.ceph.com/issues/45919", "issue_id": 45919}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay-quay-quay.apps.os.sepia.ceph.com/ceph-ci/ceph:551832b7be80c004161610d3781b826779cce471 -v bootstrap --fsid df89d676-9d3d-11ef-bb9c-d5e06f7e0c9a --config /home/ubuntu/cephtest/seed.ceph.conf --output-config /etc/ceph/ceph.conf --output-keyring /etc/ceph/ceph.client.admin.keyring --output-pub-ssh-key /home/ubuntu/cephtest/ceph.pub --mon-id a --mgr-id y --orphan-initial-daemons --skip-monitoring-stack --mon-ip 172.21.15.16 --skip-admin-label && sudo chmod +r /etc/ceph/ceph.client.admin.keyring'": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay-quay-quay.apps.os.sepia.ceph.com/ceph-ci/ceph:551832b7be80c004161610d3781b826779cce471 -v bootstrap --fsid d12a95f4-9d40-11ef-bb9c-d5e06f7e0c9a --config /home/ubuntu/cephtest/seed.ceph.conf --output-config /etc/ceph/ceph.conf --output-keyring /etc/ceph/ceph.client.admin.keyring --output-pub-ssh-key /home/ubuntu/cephtest/ceph.pub --mon-id a --mgr-id y --orphan-initial-daemons --skip-monitoring-stack --mon-ip 172.21.15.1 --skip-admin-label && sudo chmod +r /etc/ceph/ceph.client.admin.keyring'": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay-quay-quay.apps.os.sepia.ceph.com/ceph-ci/ceph:551832b7be80c004161610d3781b826779cce471 -v bootstrap --fsid cac4f556-9d40-11ef-bb9c-d5e06f7e0c9a --config /home/ubuntu/cephtest/seed.ceph.conf --output-config /etc/ceph/ceph.conf --output-keyring /etc/ceph/ceph.client.admin.keyring --output-pub-ssh-key /home/ubuntu/cephtest/ceph.pub --mon-id a --mgr-id a --orphan-initial-daemons --skip-monitoring-stack --mon-ip 172.21.15.62 --skip-admin-label && sudo chmod +r /etc/ceph/ceph.client.admin.keyring'": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay-quay-quay.apps.os.sepia.ceph.com/ceph-ci/ceph:551832b7be80c004161610d3781b826779cce471 -v bootstrap --fsid 476a0802-9d42-11ef-bb9c-d5e06f7e0c9a --config /home/ubuntu/cephtest/seed.ceph.conf --output-config /etc/ceph/ceph.conf --output-keyring /etc/ceph/ceph.client.admin.keyring --output-pub-ssh-key /home/ubuntu/cephtest/ceph.pub --mon-id a --mgr-id a --orphan-initial-daemons --skip-monitoring-stack --mon-ip 172.21.15.86 --skip-admin-label && sudo chmod +r /etc/ceph/ceph.client.admin.keyring'": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay-quay-quay.apps.os.sepia.ceph.com/ceph-ci/ceph:551832b7be80c004161610d3781b826779cce471 -v bootstrap --fsid 21b90a82-9d37-11ef-bb9c-d5e06f7e0c9a --config /home/ubuntu/cephtest/seed.ceph.conf --output-config /etc/ceph/ceph.conf --output-keyring /etc/ceph/ceph.client.admin.keyring --ssh-private-key /root/cephadm-ssh-key --ssh-signed-cert /root/cephadm-ssh-key-cert.pub --mon-id a --mgr-id a --orphan-initial-daemons --skip-monitoring-stack --mon-ip 172.21.15.89 --skip-admin-label && sudo chmod +r /etc/ceph/ceph.client.admin.keyring'": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay-quay-quay.apps.os.sepia.ceph.com/ceph-ci/ceph:551832b7be80c004161610d3781b826779cce471 -v bootstrap --fsid b2913a22-9d3f-11ef-bb9c-d5e06f7e0c9a --config /home/ubuntu/cephtest/seed.ceph.conf --output-config /etc/ceph/ceph.conf --output-keyring /etc/ceph/ceph.client.admin.keyring --output-pub-ssh-key /home/ubuntu/cephtest/ceph.pub --mon-id a --mgr-id a --orphan-initial-daemons --skip-monitoring-stack --mon-ip 172.21.15.83 --skip-admin-label && sudo chmod +r /etc/ceph/ceph.client.admin.keyring'": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay.ceph.io/ceph-ci/ceph:reef -v bootstrap --fsid 7179f3be-9a4a-11ef-bb9c-d5e06f7e0c9a --config /home/ubuntu/cephtest/seed.ceph.conf --output-config /etc/ceph/ceph.conf --output-keyring /etc/ceph/ceph.client.admin.keyring --output-pub-ssh-key /home/ubuntu/cephtest/ceph.pub --mon-id a --mgr-id y --orphan-initial-daemons --skip-monitoring-stack --mon-ip 172.21.15.50 --skip-admin-label && sudo chmod +r /etc/ceph/ceph.client.admin.keyring'": {}, ": 'rm -rf /home/ubuntu/cephtest/clone.client.0 && git clone https://git.ceph.com/ceph.git /home/ubuntu/cephtest/clone.client.0 && cd /home/ubuntu/cephtest/clone.client.0 && git checkout bc1a0ebb55bcbeb20ebefd2a76106f030b68e2bd'": {"link": "https://tracker.ceph.com/issues/65038", "issue_id": 65038}, "wait_for_clean: failed before timeout expired": {"link": "https://tracker.ceph.com/issues/58893", "issue_id": 58893}, "smithi000.front.sepia.ceph.com:  _ansible_no_log: false  changed: false  invocation:    module_args:      allow_downgrade: false      allowerasing: false      autoremove: false      bugfix: false      cacheonly: false      conf_file: null      disable_excludes: null      disable_gpg_check: false      disable_plugin: []      disablerepo: []      download_dir: null      download_only: false      enable_plugin: []      enablerepo: []      exclude: []      install_repoquery: true      install_weak_deps: true      installroot: /      list: null      lock_timeout: 30      name:      - ceph      - ceph-base      - ceph-selinux      - ceph-common      - ceph-debuginfo      - ceph-release      - libcephfs1      - ceph-radosgw      - python-ceph      - python-rados      - python-rbd      - python-cephfs      - librbd1      - librados2      - mod_fastcgi      nobest: false      releasever: null      security: false      skip_broken: false      sslverify: true      state: absent      update_cache: false      update_only: false      use_backend: auto      validate_certs: true  msg: 'Failed to download metadata for repo ''crb'': Cannot prepare internal mirrorlist:    Status code: 503 for https://mirrors.centos.org/metalink?repo=centos-crb-9-stream&arch=x86_64&protocol=https,http    (IP: 8.43.85.67)'  rc: 1  results: []": {}, "Test failure: test_prometheus (tasks.mgr.test_module_selftest.TestModuleSelftest)": {}, "Test failure: test_file_sd_command (tasks.mgr.test_prometheus.TestPrometheus)": {}, "b'2024-11-10T10:41:40.979 DEBUG:teuthology.exit:Finished running handlers'": {}, "b'2024-11-03T07:26:40.296 DEBUG:teuthology.exit:Finished running handlers'": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay.ceph.io/ceph-ci/ceph:reef -v bootstrap --fsid ca7e6c66-9a21-11ef-bb9c-d5e06f7e0c9a --config /home/ubuntu/cephtest/seed.ceph.conf --output-config /etc/ceph/ceph.conf --output-keyring /etc/ceph/ceph.client.admin.keyring --output-pub-ssh-key /home/ubuntu/cephtest/ceph.pub --mon-id a --mgr-id y --orphan-initial-daemons --skip-monitoring-stack --mon-ip 172.21.15.27 --skip-admin-label && sudo chmod +r /etc/ceph/ceph.client.admin.keyring'": {}, ": 'sudo apt-get clean'": {"link": "https://tracker.ceph.com/issues/62456", "issue_id": 62456}, "b'2024-11-11T17:03:35.858 DEBUG:teuthology.exit:Finished running handlers'": {}, "b'2024-11-04T05:23:51.207 DEBUG:teuthology.exit:Finished running handlers'": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay.ceph.io/ceph-ci/ceph:reef -v bootstrap --fsid f39f6c36-a001-11ef-bb9c-d5e06f7e0c9a --config /home/ubuntu/cephtest/seed.ceph.conf --output-config /etc/ceph/ceph.conf --output-keyring /etc/ceph/ceph.client.admin.keyring --output-pub-ssh-key /home/ubuntu/cephtest/ceph.pub --mon-id a --mgr-id y --orphan-initial-daemons --skip-monitoring-stack --mon-ip 172.21.15.50 --skip-admin-label && sudo chmod +r /etc/ceph/ceph.client.admin.keyring'": {}, "b'2024-11-03T07:43:39.321 DEBUG:teuthology.exit:Finished running handlers'": {}, "\"2024-11-03T00:13:04.932844+0000 mon.a (mon.0) 163 : cluster [WRN] Health check failed: 1 osds down (OSD_DOWN)\" in cluster log": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay-quay-quay.apps.os.sepia.ceph.com/ceph-ci/ceph:ca35a0c9c14b6742db30f02334eccb927dd290e1 -v bootstrap --fsid fe4ea36a-9a6e-11ef-bb9c-d5e06f7e0c9a --config /home/ubuntu/cephtest/seed.ceph.conf --output-config /etc/ceph/ceph.conf --output-keyring /etc/ceph/ceph.client.admin.keyring --output-pub-ssh-key /home/ubuntu/cephtest/ceph.pub --mon-ip 172.21.15.77 --skip-admin-label && sudo chmod +r /etc/ceph/ceph.client.admin.keyring'": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay-quay-quay.apps.os.sepia.ceph.com/ceph-ci/ceph:ca35a0c9c14b6742db30f02334eccb927dd290e1 -v bootstrap --fsid d8b1292e-9a6a-11ef-bb9c-d5e06f7e0c9a --config /home/ubuntu/cephtest/seed.ceph.conf --output-config /etc/ceph/ceph.conf --output-keyring /etc/ceph/ceph.client.admin.keyring --output-pub-ssh-key /home/ubuntu/cephtest/ceph.pub --mon-id a --mgr-id y --orphan-initial-daemons --skip-monitoring-stack --mon-ip 172.21.15.27 --skip-admin-label && sudo chmod +r /etc/ceph/ceph.client.admin.keyring'": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay-quay-quay.apps.os.sepia.ceph.com/ceph-ci/ceph:33ad36cd9e2a990df138c24df83f33faea5a0d79 -v bootstrap --fsid 2ebe049e-a039-11ef-bb9c-d5e06f7e0c9a --config /home/ubuntu/cephtest/seed.ceph.conf --output-config /etc/ceph/ceph.conf --output-keyring /etc/ceph/ceph.client.admin.keyring --output-pub-ssh-key /home/ubuntu/cephtest/ceph.pub --mon-id a --mgr-id y --orphan-initial-daemons --skip-monitoring-stack --mon-ip 172.21.15.22 --skip-admin-label && sudo chmod +r /etc/ceph/ceph.client.admin.keyring'": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay-quay-quay.apps.os.sepia.ceph.com/ceph-ci/ceph:33ad36cd9e2a990df138c24df83f33faea5a0d79 -v bootstrap --fsid 2324cd02-a03e-11ef-bb9c-d5e06f7e0c9a --config /home/ubuntu/cephtest/seed.ceph.conf --output-config /etc/ceph/ceph.conf --output-keyring /etc/ceph/ceph.client.admin.keyring --output-pub-ssh-key /home/ubuntu/cephtest/ceph.pub --mon-id a --mgr-id a --orphan-initial-daemons --skip-monitoring-stack --mon-ip 172.21.15.40 --skip-admin-label && sudo chmod +r /etc/ceph/ceph.client.admin.keyring'": {}, "\"2024-11-14T17:38:12.467857+0000 osd.6 (osd.6) 18 : cluster [WRN] osd.6 ep: 266 scrubber::ReplicaReservations pg[19.4]:  timeout on replica reservations (since 2024-11-14 17:38:07)\" in cluster log": {}, "\"2024-11-14T17:33:21.328529+0000 mon.a (mon.0) 208 : cluster [WRN] Health check failed: 1 osds down (OSD_DOWN)\" in cluster log": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay.ceph.io/ceph-ci/ceph:reef -v bootstrap --fsid 6bfc228a-a234-11ef-bb9f-d5e06f7e0c9a --config /home/ubuntu/cephtest/seed.ceph.conf --output-config /etc/ceph/ceph.conf --output-keyring /etc/ceph/ceph.client.admin.keyring --output-pub-ssh-key /home/ubuntu/cephtest/ceph.pub --mon-id a --mgr-id y --orphan-initial-daemons --skip-monitoring-stack --mon-ip 172.21.15.33 --skip-admin-label && sudo chmod +r /etc/ceph/ceph.client.admin.keyring'": {}, "too many values to unpack (expected 1)": {"link": "https://tracker.ceph.com/issues/69639", "issue_id": 69639}, "'DevicePath'": {"link": "https://tracker.ceph.com/issues/69067", "issue_id": 69067}, "\"2024-11-27T07:50:00.000221+0000 mon.a (mon.0) 1478 : cluster [WRN]     pg 2.1 is active+undersized+degraded, acting [2,3]\" in cluster log": {}, "b'2024-11-27T14:39:34.137 DEBUG:teuthology.exit:Finished running handlers'": {}, "['MODULE FAILURE', 'See stdout/stderr for the exact error']": {}, "Failed to reconnect to smithi000": {}, "\"2024-12-11T07:10:00.000151+0000 mon.a (mon.0) 3099 : cluster [WRN]     pg 2.23 is stuck peering for 5m, current state remapped+peering, last acting [5,6]\" in cluster log": {}, "\"2024-12-11T19:10:00.000179+0000 mon.a (mon.0) 1642 : cluster [WRN] Health detail: HEALTH_WARN 1 osds down; Degraded data redundancy: 39/435 objects degraded (8.966%), 7 pgs degraded\" in cluster log": {}, "\"2024-12-11T04:17:54.233376+0000 mon.a (mon.0) 511 : cluster [WRN] Health check failed: Degraded data redundancy: 2/6 objects degraded (33.333%), 1 pg degraded (PG_DEGRADED)\" in cluster log": {}, "Test failure: test_osd_came_back (tasks.mgr.test_progress.TestProgress)": {}, "expected valgrind issues and found none": {"link": "https://tracker.ceph.com/issues/62777", "issue_id": 62777}, "No module named 'tasks.ceph'": {"link": "https://tracker.ceph.com/issues/63522", "issue_id": 63522}, "b'2024-12-06T12:20:22.110 DEBUG:teuthology.exit:Finished running handlers'": {}, "\"2024-12-09T08:39:35.321373+0000 mon.a (mon.0) 518 : cluster [WRN] Health check failed: Reduced data availability: 1 pg peering (PG_AVAILABILITY)\" in cluster log": {}, "b'2024-12-09T17:38:19.314 DEBUG:teuthology.exit:Finished running handlers'": {}, "b'2024-12-08T12:00:38.808 DEBUG:teuthology.exit:Finished running handlers'": {}, "b'2024-12-08T12:35:31.912 DEBUG:teuthology.exit:Finished running handlers'": {}, "\"2024-12-18T19:40:00.000166+0000 mon.a (mon.0) 989 : cluster [ERR] [ERR] OSD_OUT_OF_ORDER_FULL: full ratio(s) out of order\" in cluster log": {}, "\"2024-12-19T00:30:00.000235+0000 mon.a (mon.0) 1461 : cluster [WRN]     pg 2.4 is stuck inactive for 82s, current state undersized+degraded+peered, last acting [0]\" in cluster log": {}, "Command failed (workunit test scrub/osd-scrub-snaps.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/standalone/scrub/osd-scrub-snaps.sh'": {}, "b'2024-12-19T08:06:35.528 DEBUG:teuthology.exit:Finished running handlers'": {}, "b'2024-12-19T02:56:59.080 DEBUG:teuthology.exit:Finished running handlers'": {}, "b'2024-12-19T02:23:14.601 DEBUG:teuthology.exit:Finished running handlers'": {}, "Test failure: test_selftest_command_spam (tasks.mgr.test_module_selftest.TestModuleSelftest)": {}, "\"2024-12-30T05:05:02.797343+0000 mon.a (mon.0) 304 : cluster [WRN] Health check failed: 1 failed cephadm daemon(s) (CEPHADM_FAILED_DAEMON)\" in cluster log": {"link": "https://tracker.ceph.com/issues/65728", "issue_id": 65728}, ": 'sync && sudo umount -f /var/lib/ceph/osd/ceph-7'": {"link": "https://tracker.ceph.com/issues/38059", "issue_id": 38059}, "b'2024-12-30T15:13:14.851 DEBUG:teuthology.exit:Finished running handlers'": {}, "['Failed to manage policy for boolean nagios_run_sudo: [Errno 11] Resource temporarily unavailable']": {}, "b'2024-12-29T09:55:20.228 DEBUG:teuthology.exit:Finished running handlers'": {}, "\"2024-12-30T03:24:11.404068+0000 mon.a (mon.0) 270 : cluster [WRN] Health check failed: 2 OSD(s) experiencing slow operations in BlueStore (BLUESTORE_SLOW_OP_ALERT)\" in cluster log": {}, "b'2024-12-30T11:42:45.550 DEBUG:teuthology.exit:Finished running handlers'": {}, ": 'sudo /home/ubuntu/cephtest/cephadm --image quay.ceph.io/ceph-ci/ceph:reef shell -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring --fsid 5d662de0-c62b-11ef-9d5f-a94da4efb481 -e sha1=8489eee157150fc6857a9ca4e43025c1e96f6a88 -- bash -c \\'ceph versions | jq -e \\'\"\\'\"\\'.overall | keys\\'\"\\'\"\\' | grep $sha1\\''": {}, "b'2024-12-29T10:49:56.848 DEBUG:teuthology.exit:Finished running handlers'": {}, "Failed to fetch package version from https://shaman.ceph.com/api/search/?status=ready&project=ceph&flavor=crimson&distros=centos%2F9%2Fx86_64&sha1=ef55083b46b69fa0c434ceaea9871ede386bd186": {"link": "https://tracker.ceph.com/issues/59193", "issue_id": 59193}, "Command failed (workunit test rados/test.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 CRIMSON_COMPAT=1 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/workunits/rados/test.sh --crimson'": {"link": "https://tracker.ceph.com/issues/40119", "issue_id": 40119}, "Command failed (workunit test osd/pg-split-merge.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/standalone/osd/pg-split-merge.sh'": {}, "b'2025-01-15T02:57:00.482 DEBUG:teuthology.exit:Finished running handlers'": {}, "\"2025-01-15T17:46:00.583074+0000 osd.6 (osd.6) 8 : cluster [WRN] osd.6 ep: 228 scrubber::ReplicaReservations pg[12.6]:  timeout on replica reservations (since 2025-01-15 17:45:55)\" in cluster log": {}, "b'2025-01-19T20:48:30.193 DEBUG:teuthology.exit:Finished running handlers'": {}, "\"2025-01-20T12:09:05.219058+0000 mon.a (mon.0) 587 : cluster [WRN] Health check failed: 1 failed cephadm daemon(s) (CEPHADM_FAILED_DAEMON)\" in cluster log": {}, "cannot pull file with status: requested": {"link": "https://tracker.ceph.com/issues/62811", "issue_id": 62811}, "Command failed (workunit test mon/caps.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/workunits/mon/caps.sh'": {"link": "https://tracker.ceph.com/issues/38082", "issue_id": 38082}, "b'2025-01-20T00:14:23.951 DEBUG:teuthology.exit:Finished running handlers'": {}, ": 'sudo adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 120 ceph --cluster ceph tell osd.0 flush_pg_stats'": {"link": "https://tracker.ceph.com/issues/56097", "issue_id": 56097}, "b'OSError: [Errno 28] No space left on device'": {}, "Command crashed: 'CEPH_CLIENT_ID=0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage ceph_test_rados --balance-reads --max-ops 4000 --objects 50 --max-in-flight 16 --size 4000000 --min-stride-size 400000 --max-stride-size 800000 --max-seconds 0 --op read 100 --op write 50 --op delete 50 --op snap_create 50 --op snap_remove 50 --op rollback 0 --op copy_from 0 --op write_excl 50 --pool unique_pool_0'": {"link": "https://tracker.ceph.com/issues/40740", "issue_id": 40740}, "\"grep: /var/log/ceph/932b9d14-d9d8-11ef-bb7f-bd4984dce30f/ceph.log: No such file or directory\" in cluster log": {}, "\"2025-01-24T10:50:00.000160+0000 mon.a (mon.0) 2999 : cluster [WRN]     pg 7.10 is stuck undersized for 117s, current state active+recovering+undersized+remapped, last acting [3,4]\" in cluster log": {}, "\"2025-01-23T21:32:35.922959+0000 mon.a (mon.0) 441 : cluster [WRN] Health check failed: 1 osds down (OSD_DOWN)\" in cluster log": {}, "b'2025-01-26T08:03:10.833 DEBUG:teuthology.exit:Finished running handlers'": {}, "\"2025-01-26T00:36:13.817844+0000 mon.a (mon.0) 141 : cluster [WRN] Health check failed: 1 slow ops, oldest one blocked for 30 sec, osd.0 has slow ops (SLOW_OPS)\" in cluster log": {}, "\"2025-01-21T04:03:35.385605+0000 mon.a (mon.0) 2021 : cluster [WRN] Health check failed: 1 OSD(s) experiencing slow operations in BlueStore (BLUESTORE_SLOW_OP_ALERT)\" in cluster log": {}, "b'2025-01-21T12:50:47.239 DEBUG:teuthology.exit:Finished running handlers'": {}, "Command failed (workunit test mon/mon-stretch-mode-5-mons-8-osds.sh) on smithi000 with status 22: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/workunits/mon/mon-stretch-mode-5-mons-8-osds.sh'": {}, "\"2025-01-21T04:50:00.000162+0000 mon.a (mon.0) 3638 : cluster [WRN]     pg 8.0 is active+recovering+undersized+degraded+remapped, acting [4,11]\" in cluster log": {}, "Failed to fetch package version from https://shaman.ceph.com/api/search/?status=ready&project=ceph&flavor=default&distros=ubuntu%2F22.04%2Fx86_64&sha1=3162e265ce138646560d995974c67e2a3e2008c1": {"link": "https://tracker.ceph.com/issues/59193", "issue_id": 59193}, "Failed to fetch package version from https://shaman.ceph.com/api/search/?status=ready&project=ceph&flavor=crimson&distros=centos%2F9%2Fx86_64&sha1=81553911bfa04dbc858e27068622ea12e4820990": {"link": "https://tracker.ceph.com/issues/59193", "issue_id": 59193}, "b'2025-01-13T15:10:47.708 DEBUG:teuthology.exit:Finished running handlers'": {}, "\"2025-01-13T10:00:00.000210+0000 mon.a (mon.0) 1468 : cluster [WRN]     osd.8 (root=default,host=smithi000) is down\" in cluster log": {}, "b'2025-01-12T13:33:17.446 DEBUG:teuthology.exit:Finished running handlers'": {}, "\"2025-01-13T06:39:44.189660+0000 mon.a (mon.0) 405 : cluster [WRN] Health check failed: 1 failed cephadm daemon(s) (CEPHADM_FAILED_DAEMON)\" in cluster log": {}, "b'2025-01-27T20:43:25.990 DEBUG:teuthology.exit:Finished running handlers'": {}, "Failed to fetch package version from https://shaman.ceph.com/api/search/?status=ready&project=ceph&flavor=crimson&distros=centos%2F9%2Fx86_64&sha1=84d0301cdf900893da134262f92a840e42ff962b": {"link": "https://tracker.ceph.com/issues/59193", "issue_id": 59193}, "Command crashed: 'CEPH_CLIENT_ID=0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage ceph_test_rados --localize-reads --max-ops 4000 --objects 50 --max-in-flight 16 --size 4000000 --min-stride-size 400000 --max-stride-size 800000 --max-seconds 0 --op read 100 --op write 50 --op delete 50 --op snap_create 50 --op snap_remove 50 --op rollback 0 --op copy_from 0 --op write_excl 50 --pool unique_pool_0'": {"link": "https://tracker.ceph.com/issues/40740", "issue_id": 40740}, ": 'sudo adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 120 ceph --cluster ceph tell osd.1 flush_pg_stats'": {"link": "https://tracker.ceph.com/issues/56097", "issue_id": 56097}, "MAX_BACKTRACE_LINES": {}, "ONE OF THE TESTS HAVE A BACKTRACE": {"link": "https://tracker.ceph.com/issues/69428", "issue_id": 69428}, "\"1738461364.985674 mon.a (mon.0) 505 : cluster [WRN] Health check failed: Reduced data availability: 1 pg peering (PG_AVAILABILITY)\" in cluster log": {}, "Command failed (workunit test mon/osd-erasure-code-profile.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/standalone/mon/osd-erasure-code-profile.sh'": {}, "Command failed (workunit test erasure-code/test-erasure-code.sh) on smithi000 with status 1: 'mkdir -p -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && cd -- /home/ubuntu/cephtest/mnt.0/client.0/tmp && CEPH_CLI_TEST_DUP_COMMAND=1 CEPH_REF=XXXXXXXXXXXXXXXXXX TESTDIR=\"/home/ubuntu/cephtest\" CEPH_ARGS=\"--cluster ceph\" CEPH_ID=\"0\" PATH=$PATH:/usr/sbin CEPH_BASE=/home/ubuntu/cephtest/clone.client.0 CEPH_ROOT=/home/ubuntu/cephtest/clone.client.0 CEPH_MNT=/home/ubuntu/cephtest/mnt.0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage timeout 3h /home/ubuntu/cephtest/clone.client.0/qa/standalone/erasure-code/test-erasure-code.sh'": {}, "\"2025-02-02T00:17:13.071846+0000 osd.1 (osd.1) 3 : cluster [WRN] Monitor daemon marked osd.1 down, but it is still running\" in cluster log": {}, "b'2025-02-02T22:10:20.786 DEBUG:teuthology.exit:Finished running handlers'": {}, "\"2025-01-27T10:40:00.000182+0000 mon.a (mon.0) 1971 : cluster [WRN]     pg 4.a is active+undersized+degraded+remapped+backfilling, acting [10,7]\" in cluster log": {}, "\"2025-02-02T18:29:27.232403+0000 mon.a (mon.0) 174 : cluster [WRN] Health check failed: 1 slow ops, oldest one blocked for 30 sec, osd.0 has slow ops (SLOW_OPS)\" in cluster log": {}, "Command crashed: 'CEPH_CLIENT_ID=0 adjust-ulimits ceph-coverage /home/ubuntu/cephtest/archive/coverage ceph_test_rados --write-fadvise-dontneed --max-ops 4000 --objects 500 --max-in-flight 16 --size 4000000 --min-stride-size 400000 --max-stride-size 800000 --max-seconds 0 --op read 100 --op write 50 --op delete 10 --op write_excl 50 --pool unique_pool_0'": {"link": "https://tracker.ceph.com/issues/40740", "issue_id": 40740}}